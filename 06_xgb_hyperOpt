import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from hyperopt import hp, fmin, tpe, Trials, STATUS_OK
from sklearn.metrics import r2_score

# Your existing code for data preparation
df = pd.read_csv('dataset_v1.csv')
X = df.drop('Y', axis=1)
X = X.drop(columns=["assignm", "funcCalls"])
y = df['Y']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the hyperparameter space
space = {
    'max_depth': hp.choice('max_depth', range(3, 10)),
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.5),
    'n_estimators': hp.choice('n_estimators', range(50, 300)),
    'min_child_weight': hp.choice('min_child_weight', range(1, 6)),
    'subsample': hp.uniform('subsample', 0.5, 1)
}

# Define the objective function
def objective(params):
    model = XGBRegressor(**params, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    score = r2_score(y_val, y_pred)
    return {'loss': -score, 'status': STATUS_OK}

# Run the optimization
trials = Trials()
best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)

print("Best parameters:", best_params)
